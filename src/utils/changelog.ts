/**
 * Changelog å·¥å…·å‡½æ•°
 * ä» Git å†å²è·å–æœ€è¿‘æäº¤ï¼Œå¹¶ä½¿ç”¨ LLM ç”Ÿæˆä¸­æ–‡æ‘˜è¦
 */
import { execSync } from 'node:child_process'
import { readFileSync, writeFileSync, existsSync } from 'node:fs'

// ============ Types ============

export interface CommitInfo {
  hash: string
  date: string
  message: string
  summary?: string // LLM ç”Ÿæˆçš„æ‘˜è¦
}

interface CacheData {
  [hash: string]: string // hash -> summary
}

// ============ Config ============

const CACHE_FILE = '.changelog-cache.json'
const MAX_COMMITS = 8

// ============ Git Functions ============

/**
 * è·å–æœ€è¿‘ N æ¡ Git æäº¤
 */
export function getRecentCommits(count: number = MAX_COMMITS): CommitInfo[] {
  try {
    // æ ¼å¼: hash|date|message
    const output = execSync(
      `git log -${count} --pretty=format:"%H|%ad|%s" --date=short`,
      { encoding: 'utf-8', stdio: ['pipe', 'pipe', 'pipe'] },
    )

    return output
      .trim()
      .split('\n')
      .filter(Boolean)
      .map((line) => {
        const [hash, date, ...messageParts] = line.split('|')
        return {
          hash: hash.substring(0, 7), // çŸ­ hash
          date,
          message: messageParts.join('|'), // å¤„ç† message ä¸­å¯èƒ½åŒ…å« | çš„æƒ…å†µ
        }
      })
  } catch (error) {
    console.error('Failed to get git commits:', error)
    return []
  }
}

// ============ Cache Functions ============

function loadCache(): CacheData {
  try {
    if (existsSync(CACHE_FILE)) {
      return JSON.parse(readFileSync(CACHE_FILE, 'utf-8'))
    }
  } catch {
    // ignore
  }
  return {}
}

function saveCache(cache: CacheData): void {
  try {
    writeFileSync(CACHE_FILE, JSON.stringify(cache, null, 2))
  } catch {
    // ignore
  }
}

// ============ LLM Functions ============

/**
 * è°ƒç”¨ LLM API ç”Ÿæˆæäº¤æ‘˜è¦
 */
async function callLLM(messages: string[]): Promise<string[]> {
  const apiUrl = import.meta.env.LLM_API_URL || process.env.LLM_API_URL
  const apiKey = import.meta.env.LLM_API_KEY || process.env.LLM_API_KEY
  const model =
    import.meta.env.LLM_MODEL || process.env.LLM_MODEL || 'gpt-3.5-turbo'

  if (!apiUrl || !apiKey) {
    console.log('LLM API not configured, using original messages')
    return messages
  }

  const prompt = `ä½ æ˜¯ä¸€ä¸ª Git æäº¤ä¿¡æ¯ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ Git æäº¤ä¿¡æ¯ç¿»è¯‘æˆç®€æ´çš„ä¸­æ–‡æ‘˜è¦ã€‚
è¦æ±‚ï¼š
1. æ¯æ¡æ‘˜è¦ä¸è¶…è¿‡ 30 ä¸ªå­—
2. ä½¿ç”¨åˆé€‚çš„ emoji å¼€å¤´ï¼ˆå¦‚ âœ¨æ–°åŠŸèƒ½ã€ğŸ›ä¿®å¤ã€ğŸ“æ–‡æ¡£ã€ğŸ¨æ ·å¼ã€âš¡æ€§èƒ½ã€ğŸ”§é…ç½®ï¼‰
3. ä¿æŒæŠ€æœ¯å‡†ç¡®æ€§
4. æ¯è¡Œä¸€æ¡ï¼ŒæŒ‰é¡ºåºè¾“å‡º

æäº¤ä¿¡æ¯ï¼š
${messages.map((m, i) => `${i + 1}. ${m}`).join('\n')}

è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œæ¯è¡Œä¸€æ¡ï¼š`

  try {
    // ç¡®ä¿ URL æ ¼å¼æ­£ç¡®
    const baseUrl = apiUrl.endsWith('/') ? apiUrl.slice(0, -1) : apiUrl
    const endpoint = baseUrl.includes('/v1')
      ? `${baseUrl}/chat/completions`
      : `${baseUrl}/v1/chat/completions`

    const response = await fetch(endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${apiKey}`,
      },
      body: JSON.stringify({
        model,
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3,
        max_tokens: 500,
      }),
    })

    if (!response.ok) {
      throw new Error(`LLM API error: ${response.status}`)
    }

    const data = await response.json()
    const content = data.choices?.[0]?.message?.content || ''

    // è§£æè¿”å›çš„æ‘˜è¦
    const summaries = content
      .trim()
      .split('\n')
      .map((line: string) => line.replace(/^\d+\.\s*/, '').trim())
      .filter(Boolean)

    // ç¡®ä¿è¿”å›æ•°é‡åŒ¹é…
    return messages.map((_, i) => summaries[i] || messages[i])
  } catch (error) {
    console.error('LLM API call failed:', error)
    return messages
  }
}

// ============ Main Export ============

/**
 * è·å–å¸¦æœ‰ LLM æ‘˜è¦çš„ changelog
 */
export async function getChangelog(): Promise<CommitInfo[]> {
  const commits = getRecentCommits()
  if (commits.length === 0) return []

  const cache = loadCache()

  // æ‰¾å‡ºéœ€è¦ç”Ÿæˆæ‘˜è¦çš„æäº¤
  const uncachedCommits = commits.filter((c) => !cache[c.hash])

  if (uncachedCommits.length > 0) {
    const messages = uncachedCommits.map((c) => c.message)
    const summaries = await callLLM(messages)

    // æ›´æ–°ç¼“å­˜
    uncachedCommits.forEach((commit, i) => {
      cache[commit.hash] = summaries[i]
    })
    saveCache(cache)
  }

  // è¿”å›å¸¦æ‘˜è¦çš„æäº¤åˆ—è¡¨
  return commits.map((commit) => ({
    ...commit,
    summary: cache[commit.hash] || commit.message,
  }))
}

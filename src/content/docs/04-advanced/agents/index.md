---
title: 'Agents'
description: '从聊天到动手干活给 LLM 装上手脚'
contributors: [codex]
---


More Details： 
- [Hello-Agents by Datawhale](https://github.com/datawhalechina/hello-agents)
- [Mini-Agent](https://github.com/MiniMax-AI/Mini-Agent)


LLM 只是预测下一个 token 的统计机，它既看不到真实世界状态，也不能自己碰操作系统。想让它真正把事情做好，就得把“想”和“做”拆开：模型负责推理，Agent 负责把推理变成可执行的工具调用。

**关键结论**
- 纯 LLM 无法直接执行任务，因为它没有权限、没有可观测环境，也没有持久状态。
- 函数调用 / MCP 把模型输出转成结构化工具调用，把“想法”嫁接到真实的 API、命令行和浏览器上。
- Agent 循环就是“思考 → 调用 → 观测 → 反思”的闭环，不断重复直到目标满足或被判失败。
- 记忆系统把对话窗口和持久存储结合，才让多步任务、上下文切换和长时间运行成为可能。

## 为什么 LLM 不能直接执行任务？

| 限制 | 为什么存在 | 带来的问题 |
| --- | --- | --- |
| 沙盒输出 | 模型只能返回文本，不能触碰文件或网络 | 任何副作用都需要人工复制粘贴 |
| 无环境感知 | 训练时看不到实时文件、日志、接口 | 看不到命令结果，只能猜 |
| 无持久状态 | 上下文窗口有限，旧信息被截断 | 长链路任务凭空丢线索 |
| 概率推理 | 输出是概率分布，缺乏确定性 | 不适合直接驱动不可逆操作 |

这四个限制决定了：没有额外外设，LLM 永远是“嘴炮型助手”，再聪明也只是讲故事。

## 函数调用如何把“嘴炮”变成“动手”

函数调用（Function Calling）和 MCP 约定了一个契约：模型必须返回结构化 JSON，Agent 只要老老实实执行就行。流程分成三个最小组件：

1. **Schema**：定义可用工具、参数、返回值，阻止模型胡来。
2. **调用**：LLM 依据 schema 填 JSON，如 `{"name":"bash","arguments":{"command":"ls"}}`。
3. **执行与回传**：Agent Runtime 验证请求、执行真实工具，把 stdout/stderr 以文本或二进制再塞回模型上下文。

这个契约的好处是：

- **可控安全面**：只暴露必要工具，必要时沙盒执行。
- **可观测**：所有输入输出都留在上下文里，模型可以复盘。
- **可插拔**：换浏览器、数据库、CI 只是换工具实现，不动模型。

## Agent 工作循环：思考与执行的配速器

一个最小 Agent 只干三件事：维护目标、调度工具、在错误前收手。整个 ReAct 循环可以拆成以下链路：
1. **Goal Intake**：解析用户目标，抽出约束（截止时间、目录、接口）。
2. **Plan or Scratchpad**：把大目标拆成最小可验证的行动，比如“先读文件 → 再改 → 再测”。
3. **Tool Selection**：基于当前状态挑一个工具，生成结构化调用。
4. **Execution**：Agent Runtime 在真实环境执行，收集 stdout/stderr、返回码、diff。
5. **Observation**：结果被塞回模型上下文，模型判断是否达成子目标。
6. **Control**：成功就继续下一步，失败就回滚、换策略或宣告任务失败。

**循环示意（ASCII 流程图）**

```txt
            用户          模型         Agent框架       工具         记忆
            |             |              |             |            |
            |--描述任务--->|              |             |            |
            |             |              |             |            |
            |             |--工具调用--->|              |            |
            |             |   (JSON)     |             |            |
            |             |              |             |            |
            |             |              |--执行/沙盒-->|            |
            |             |              |             |            |
            |             |              |<---结果-----|             |
            |             |              |             |            |
            |             |              |--------写入观测---------->|
            |             |              |             |            |
            |             |<--观测/状态---|             |            |
            |             |              |             |            |
            |<---答案-----|               |             |            |
            |     or      |              |             |            |
            |<--追加请求---|              |             |            |
            |             |              |             |            |
```

没有这个闭环，模型永远不知道命令到底跑没跑、文件有没有写对；有了闭环，模型就能像程序员一样“先想，再做，再读结果”。

## 记忆系统让多步任务可落地

Agent 需要两级记忆，否则循环只能跑几步就失忆：

### 1. 短期记忆：上下文窗口 + Scratchpad

- **对话窗口**：保存最近的工具调用、结果、错误栈。
- **结构化 Scratchpad**：比如“计划表”“调试日志”，让模型在窗口里有稳定锚点。
- **归纳压缩**：窗口爆满前，先把多次交互压成总结，再继续写入新事实。

### 2. 长期记忆：外部存储

- **向量数据库**：把阶段性成果 embedding，下次检索回来对齐语义。
- **状态文件 / KV**：存精确的中间产物（配置、待办、环境信息）。
- **检查点策略**：定期把计划、上下文快照写盘，崩溃后能恢复。

短期记忆保证“最近一步”不丢，长期记忆保证“长链路目标”不散，两者配合才能让多小时、多天的任务继续推进，而不是每次都从零开始。

## 把一切串起来

1. 没有工具，LLM 只能输出文本；函数调用把输出变成可执行指令。
2. 没有循环控制，模型无法学会“试错”；Agent Runtime 负责执行、校验、把观测送回去。
3. 没有记忆，多步任务必崩；两级记忆让模型既能关注眼前，也能记住长远约束。

这三块拼起来才是真正的 Agent：模型想，Runtime 做，记忆存。只要任意一块缺席，所谓“智能体”都会退化成花哨的聊天机器人。

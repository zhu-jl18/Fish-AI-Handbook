---
title: LLM 越狱攻防
description: LLM 越狱攻防系列文章，探索大语言模型安全边界与防护策略
contributors:
  - codex
---

# LLM 越狱攻防

> 本系列共五篇文章，系统性地探讨大语言模型（LLM）越狱攻击与防御的技术原理。

## 系列导读

<!-- 你可以在这里添加系列介绍内容 -->

## 参考资料

- [AI 大脑如何被"套路"?—揭秘大模型提示词攻防（含防御策略部分，FreeBuf 转载版）](https://www.freebuf.com/articles/endpoint/432798.html)
- [How Microsoft discovers and mitigates evolving attacks against AI guardrails（Microsoft Security Blog）](https://www.microsoft.com/en-us/security/blog/2024/04/11/how-microsoft-discovers-and-mitigates-evolving-attacks-against-ai-guardrails/)
- [How LLMs can be compromised in 2025 – New types of attacks on AI-powered assistants and chatbots（Kaspersky）](https://www.kaspersky.com/blog/new-llm-attack-vectors-2025/54323/)
- [Constitutional Classifiers: Defending against universal jailbreaks（Anthropic 研究页面）](https://www.anthropic.com/research/constitutional-classifiers)
- [Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming（arXiv）](https://arxiv.org/abs/2501.18837)
- [GuidedBench: Equipping Jailbreak Evaluation with Guidelines（arXiv）](https://arxiv.org/html/2502.16903v1)
- [AI-Safety_Benchmark / GuidedBench 官方仓库（GitHub）](https://github.com/SproutNan/AI-Safety_Benchmark)
- [Jailbreaking LLMs: A Comprehensive Guide (With Examples)（Promptfoo 博客，用于红队测试自动化）](https://www.promptfoo.dev/blog/how-to-jailbreak-llms/)
- [r/LocalLLaMA（Reddit 社区，大量本地模型与越狱讨论）](https://www.reddit.com/r/LocalLLaMA/)
- [OpenPrompt（ChatGPT / LLM 提示词分享站，含越狱与角色扮演类提示）](https://openprompt.co/)

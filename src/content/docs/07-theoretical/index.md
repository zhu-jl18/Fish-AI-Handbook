---
title: 理论学习
description: AI 相关理论知识与学术研究，包括算法原理、训练方法等内容
contributors: [gemini]
---

## 为什么要了解“理论”？

我们不是为了写论文，也不是为了从头训练一个基座模型。
作为一个大模型爱好者或深度用户，了解理论是为了 **“懂它”**。

*   **祛魅**：它不是魔法，也不是真人。了解它的原理，你就能理解它为什么会一本正经地胡说八道。
*   **共情**：了解它的训练过程，你就能明白为什么它有时候需要你“一步步思考”，为什么它对某些格式特别敏感。
*   **直觉**：建立正确的 **Mental Model（心智模型）**，你写 Prompt 时就不再是盲目尝试，而是基于对它思考方式的理解。

---

## 知识地图：从微观到宏观

本章内容按照从 **模型底层** 到 **系统应用** 的逻辑编排：

### 1. 基座模型 (The Engine)
这是 LLM 的物理引擎，决定了“智力”的上限。
*   [**Transformer**](./transformer/)：现代 LLM 的心脏。不讲公式，只讲 Self-Attention 到底在“看”什么，以及它如何处理长距离依赖。
*   [**MoE (Mixture of Experts)**](./moe/)：如何在不增加推理成本的情况下把模型做大？稀疏激活是当前大模型（如 GPT-4, DeepSeek, Mixtral）的标配。
*   [**Scaling Laws**](./scaling-laws/)：AI 时代的摩尔定律。它指导我们如何分配算力、数据和参数量，是预训练的“投资指南”。

### 2. 训练与对齐 (The Education)
有了脑子，还得教它怎么说话，怎么做人。
*   [**TR & TL (Token Ratio & Transfer Learning)**](./tr-and-tl/)：数据配比的艺术。代码写得好，数学也能变好？不同领域的数据如何相互促进或干扰。
*   [**Alignment (SFT & RLHF)**](./alignment/)：如何让模型听话。从指令微调到人类偏好对齐，探讨如何抑制幻觉、增加安全性。
*   [**GRPO**](./grpo/)：DeepSeek-R1 背后的强化学习优化算法，如何在没有 Value Network 的情况下高效优化策略。

### 3. 系统与应用 (The System)
把模型封装成解决问题的 Agent。
从训练与数据的视角，解构“智能体”能力的来源。
*   [**Agents**](./agents/)：Agent 能力并非魔法，而是来自特定的训练流水线。探讨 SFT 轨迹学习、强化学习（RL）如何赋予模型规划与工具调用能力，以及 Runtime 在其中的作用。

---

## 快速索引 (Cheat Sheet)

| 章节 | 核心问题 | 快速收获 |
| :--- | :--- | :--- |
| **Transformer** | 注意力是否足以覆盖长程依赖 | multi-head、残差、归一化的分工 |
| **MoE** | 为什么稀疏化能在固定算力内提升容量 | gating 与负载均衡对路由稳定性的影响 |
| **Scaling Laws** | 数据/参数/算力的幂律如何指导投入 | 估算性能收益、判断何时该扩容 |
| **Alignment** | 如何把语言预测器约束到人类目标 | 指令微调、偏好数据、红线策略的关系 |
| **GRPO** | 偏好优化到底在优化什么梯度 | 奖励建模、策略更新与稳定性要点 |
| **TR & TL** | ICL 是在“认任务”还是在“学任务” | 如何根据任务选择标签语义与示例设计 |
| **Agents** | Agent 能力是模型自带的吗 | SFT 学习格式，RL 强化策略，Runtime 兜底安全 |

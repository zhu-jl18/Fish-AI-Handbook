---
title: 上下文学习的本质：TR 与 TL
description: 别把 LLM 当魔法看，深入解剖 Task Recognition 和 Task Learning 两种机制，理解为什么大模型能从上下文里“学习”。
contributors: [codex,gemini]
---

听着，我不管营销号怎么吹嘘大模型有“意识”或者“魔法”，在代码和数学面前，这些都是废话。如果你想真正理解为什么把一堆示例扔进 Prompt 里（所谓的 In-Context Learning, ICL），模型就能像变戏法一样给你吐出正确答案，你就得把脑子里的那些玄学清空。

这背后只有两个极其无聊但绝对核心的机制在起作用：**任务识别（Task Recognition, TR）** 和 **任务学习（Task Learning, TL）**。

把这两个概念搞清楚，你才配谈 Prompt Engineering。否则你只是在瞎猫碰死耗子。

## 0. 核心概念：别被术语吓到了

Princeton 和 Google Research 那帮人（Pan, Wei 等）至少有一件事是做对了：他们把 ICL 拆成了两部分，终于让这玩意儿从“玄学”变回了“工程学”。

### 任务识别 (Task Recognition, TR)

**本质：** 这是一个“检索 / 模式匹配”过程。

模型是个懒鬼。当你给它几个示例时，它首先做的是通过这些词汇（比如“电影”“太棒了”“太烂了”）去匹配它预训练权重里已经见过的任务模式。

- **表现**：它认出了“哦，这是情感分类”。
- **依赖**：完全依赖**预训练先验 (Priors)**。
- **行为**：它根本不在乎你示例里的标签是对是错。你告诉它“这电影太棒了 -> 负面”，它大概率还是会输出“正面”，因为它不信你，它信它训练过的几个 T 的数据。

### 任务学习 (Task Learning, TL)

**本质：** 这是一个“拟合 / 学习”过程。

这才是真正的 In-Context Learning。模型被迫放弃它的固有偏见，盯着你给的那几个 $(x, y)$ 映射，在推理的瞬间，在它的激活空间（activation space）里**隐式训练出一个新的小模型 $f(x)$**。

- **表现**：它学会了“在这个上下文里，‘棒’对应的是‘0’，‘烂’对应的是‘1’”。
- **依赖**：完全依赖**上下文映射 (input–label mappings)**。
- **行为**：它具备了覆盖先验的能力。你定义什么规则，它就信什么规则，哪怕这个规则是反直觉的。

ICL 这坨东西，99% 的玄学争论都可以翻译成一个问题：

> “眼前这个模型，现在是在跑 TR，还是在跑 TL，或者两者掺着来？”

## 1. 实验证据：数据不会撒谎

Pan et al. 2023 专门设计了一个看上去很蠢，但实验价值拉满的套路：Gold / Random / Abstract 三种 prompt 设定，用来拆开 TR 和 TL 的贡献。

### 实验设置对照表

| 模式 | 示例内容 (Prompt) | 标签真实性 | 想测什么 | 怎么解读 |
| :--- | :--- | :--- | :--- | :--- |
| **Gold (标准)** | `Input: 好评, Label: Positive` | ✅ 正确 | **TR+TL 综合** | 这是你平时用的 few-shot，作为整体 ICL 性能基线。 |
| **Random (随机)** | `Input: 好评, Label: Negative`（每个样本标签随机打乱） | ❌ 错误 | **主要测 TR** | 映射关系被打碎，TL 没法学；如果还不差，说明模型靠的是“认任务+预训练先验”。 |
| **Abstract (抽象)** | `Input: 好评, Label: Foo`（Foo/Bar 这种无语义符号） | ✅ 正确，但无语义 | **主要测 TL** | 标签没有语义，TR 靠不住；如果还能学会好评→Foo，差评→Bar，只能说明它真的在上下文里拟合了一个新函数。 |

### 结论是什么？

1. **Random 模式下依然能跑：TR 单独就能撑起一片天。**
   
   很多模型（尤其是小模型）在 Random 设定下仍然明显好于随机：
   
   - 显然不可能是 TL，因为标签是乱的，根本不存在稳定映射可以学；
   - 唯一解释：它完全无视你的标签，只根据输入内容 + 预训练知识直接产出“看起来对的答案”。
   
   这就是“只认任务、不学映射”的 TR 在干活。

2. **Abstract 模式下只有大模型能玩：TL 是涌现的。**
   
   - 标签变成 `A/B`、`Foo/Bar` 之后，小模型几乎崩盘，准确率靠近随机；
   - 大模型随示例数增加，准确率稳步爬升，在不少任务上能学出一个像样的分类器。
   
   这说明：**只有大模型在上下文里真的在“学一个新函数”，而不是在回忆先验。**

## 2. 规模效应：大模型和小模型的真正鸿沟

Pan 2023 + Wei 2023 都在不同角度证实了同一件事：

- **TR 在小模型上就不差**，而且随着模型变大 / 示例变多，提升有限；
- **TL 明显是随规模涌现的能力**，只有到了一定规模以后，模型才开始在 Abstract / flipped-label 这类设定下表现出“覆盖先验、学新规则”的特性。

可以用一张极简的字符图记住这件事：

```text
准确率 (Performance)
  ^
  |                                            /
  |                                          /   <-- TL (任务学习)
  |                                        /       随模型规模 & 示例数稳定提升
  |                                      /         只有大模型才有的本事
  |                                    /
  |                                  /
  |------------------------------/--------------  <-- TR (任务识别)
  |                            /                  小模型就能达到的上限
  |                          /                    示例 / 规模再堆也涨不动多少
  |                        /
  |______________________/___________________>   模型规模 / few-shot 示例数
     小模型                                       大模型
```

翻译成人话就是：

- 小模型：**认任务可以，学新规则别想。**
- 大模型：**既能认任务，又能在上下文里拟合新 mapping。**

## 3. 深入底层：Transformer 里到底发生了什么？

前面讲的都是“功能级别”的分析。2025 年的 *Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis* 又往前走了一步：它在**注意力头 (attention head)** 粒度上，把 TR 和 TL 的“物理位置”给掏出来了。

极度简化地说：

1. **任务子空间 (task subspace)**
   
   - 把所有候选标签 token 的反嵌入向量（unembedding）看作张成一个低维子空间 $S$；
   - 比如情感任务里，“positive/negative/neutral” 的向量就大致张成一个情感子空间。

2. **TR 头：把状态“拉进”任务子空间**
   
   - 分析每个 head 的输出，算它在 $S$ 上的投影；
   - 一类 head 的主要效果是：**增加 hidden state 在 $S$ 上的能量**，也就是把表示往“标签所在的那块空间”推；
   - 这些 head 就是所谓的 **TR heads**：
     
     > 功能：告诉模型“现在要在这些标签里面选一个，不要乱跑到别的 token 去”。

3. **TL 头：在子空间里旋转到正确标签方向**
   
   - 另一类 head 的输出，在任务子空间内部，更接近于“正确标签向量 − 错误标签向量”的方向；
   - 这些 head 负责**在 $S$ 里精细地区分哪个标签才是对的**；
   - 它们就是 **TL heads**：
     
     > 功能：根据上下文示例，在标签子空间内学一个决策边界，把不同输入送到不同标签方向上。

4. **消融验证**
   
   - 禁用 TR 头 → Random / Gold 设定下性能大幅下降，而 Abstract 受影响相对较小；
   - 禁用 TL 头 → Abstract / flipped-label 场景下直接爆炸，而普通 Gold 减少没那么惨；
   - 模式和 “TR 主要靠先验、TL 主要靠映射” 的故事高度一致。

换句话说：

> TR = 有一拨 head 在把状态往“该任务的标签空间”里拉；  
> TL = 另一拨 head 在这个空间里，把不同样本转到不同标签方向。

## 4. 工程师指南：怎么利用这个机制写 Prompt？

懂了这些，你就不该再“感觉写 Prompt”，而是有目的地压 TR 或压 TL。

下面是一个精简版的“工程师视角”总结表：

| 特性 | Task Recognition (TR) | Task Learning (TL) |
| :--- | :--- | :--- |
| **核心机制** | 模式匹配与检索（选任务、选标签空间） | 在激活里训练一个隐式模型（拟合 $x \to y$） |
| **依赖来源** | 预训练数据的先验知识 (priors) | 上下文中的示例 (demonstrations) |
| **对标签敏感度** | 低：标签乱写，它照样能凭语义先验给出“差不多的答案” | 高：标签乱写就学歪，TL 直接报废 |
| **所需模型规模** | 小模型就能表现明显 TR | 需要较大模型（通常数十亿参数以上）才会稳定涌现 TL |
| **本质** | “回忆” (recalling) | “学习” (acquiring) |

### 场景 A：标准任务（翻译、情感分析、常规分类）

这种任务人类早就玩烂了，互联网语料里到处都是对应模式。对大模型也是一样。

- **策略：压 TR，别折腾 TL。**
- **标签设计**：
  - 用有语义的自然语言标签，例如 `positive / negative`、`entailment / contradiction`；
  - 不要一上来就搞 `0/1`、`A/B` 这种没语义的鬼东西。
- **示例作用**：
  - 主要是告诉模型：“现在做的是情感分类 / NLI / 翻译，这就是输出格式”；
  - 具体哪个句子是好评，哪个是差评，模型本身早就知道了。
- **对示例质量的要求**：
  - 格式别写乱，语义别太离谱就行；
  - 少量标注错误，模型往往会被自己的先验“修正”回来。

### 场景 B：自定义业务规则（内部标签、怪异标准、反直觉逻辑）

比如：

- 你公司内部定义了一套怪异的工单优先级（P0/P1/P2 的含义和公认标准完全不同）；
- 你想让模型在上下文里学会“这种文本 → 内部标签 ID”的映射，而这些 ID 在互联网上几乎没有公开语义。

这种情况下：

- **策略：强制压 TL。**
- **标签设计**：
  - 优先用**语义弱 / 无语义**的标签：`Class_A / Class_B`、业务 ID、emoji 等；
  - 目的是让模型**不能偷懒用语义先验**，只能老老实实看你给的示例学规则。
- **示例要求**：
  - 覆盖各种典型样本（边界情况、多种说法）；
  - 映射必须高度一致，宁可少也别错。
- **模型选择**：
  - 这类任务本质上就是要 TL，**小模型通常顶不住**；
  - 你需要的是“有明显 TL 涌现”的大模型——至于是哪个版本、哪家厂商，随着时间会变，不要在文档里写死具体名字。

## 5. 总结

把花里胡哨的产品名和营销词都扔掉，ICL 落到地面上就两件事：

- **TR (Task Recognition)**：模型在**查字典**。
  - 认出任务是什么、标签空间在哪，然后凭预训练先验“按常理”作答；
  - 小模型就能干得不错，规模再堆收益有限。

- **TL (Task Learning)**：模型在**做回归 / 训练隐式模型**。
  - 在激活空间里基于上下文样本“训练”一个 $x \to y$ 的新映射；
  - 需要足够大的模型和足够多的示例，是一种随规模涌现的能力。

写 Prompt 的时候，如果你连自己到底是想触发 TR 还是 TL 都说不清楚，那基本就是瞎搞。

**问自己三个问题：**

1. 这个任务，模型在预训练里是不是已经见过类似的？
2. 我现在给的示例，是主要在告诉它“这是个什么任务”（TR），还是在教它“这里有一套新规则”（TL）？
3. 我的模型规模，够不够支撑我期望的 TL 行为？

想清楚这几个，再动手写 Prompt，省你一堆玄学调参时间。

## 6. 参考资料 (References)

下面这些是真正支撑本文观点的技术论文，全部是公开可查、且和 TR/TL 机制直接相关的研究：

- **Pan et al., 2023 — What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning**  
  *Venue*: Findings of ACL 2023  
  *Summary*: 首次系统性提出 TR / TL 概念，并通过 Gold / Random / Abstract 三种 prompt 设定将二者在实验上拆分开来，展示了 TR 在小模型就很强，而 TL 随模型规模和示例数涌现。  
  [arXiv:2305.09731](https://arxiv.org/abs/2305.09731)

- **Wei et al., 2023 — Larger language models do in-context learning differently**  
  *Venue*: arXiv preprint, Google Research  
  *Summary*: 通过“翻转标签 (flipped labels)”和“语义无关标签 (SUL-ICL)”说明：小模型主要依赖语义先验（更像 TR），而大模型才具备覆盖先验、真正学习新映射（TL）的能力。  
  [arXiv:2303.03846](https://arxiv.org/abs/2303.03846)

- **Akyürek et al., 2022 — What learning algorithm is in-context learning? Investigations with linear models**  
  *Venue*: ICLR 2023  
  *Summary*: 在线性回归等简单任务上，证明 Transformer 可以在激活空间里实现显式的梯度下降 / 岭回归，从而把 ICL 视作“在上下文中训练一个隐式模型”的算法过程。  
  [arXiv:2211.15661](https://arxiv.org/abs/2211.15661)

- **Yang et al., 2025 — Localizing Task Recognition and Task Learning in In-Context Learning via Attention Head Analysis**  
  *Venue*: arXiv preprint (2025)  
  *Summary*: 在注意力头粒度上定位出“TR 头”和“TL 头”，从几何角度说明前者负责将状态拉入任务子空间，后者在子空间内将状态旋转到正确标签方向，为 TR/TL 提供了模型内部的机制解释。  
  [arXiv:2509.24164](https://arxiv.org/abs/2509.24164)

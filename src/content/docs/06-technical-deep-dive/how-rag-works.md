---
title: "RAG 工作原理解析"
description: "深入剖析检索增强生成（RAG）的工作流程、核心组件与关键挑战。"
---

## RAG 的诞生：为何需要它？

大语言模型（LLM）虽然知识渊博，但面临两大核心挑战：
1.  **知识截止 (Knowledge Cutoff)**：模型的知识停留在训练数据的某个时间点，无法获取最新信息。
2.  **幻觉 (Hallucination)**：模型有时会“一本正经地胡说八道”，编造一些看似合理但实际错误的信息，尤其是在处理其知识范围外或需要高事实性的问题时。

**RAG (检索增强生成)** 正是为了解决这些问题而设计的框架。它的核心思想非常直观：**不让模型仅凭记忆回答，而是给它一本书（外部知识库）让它边查边答。**

通过在生成答案之前，先从一个最新的、可信的知识库中检索相关信息，RAG 能够显著提升答案的准确性、时效性，并有效抑制幻觉。

## RAG 的核心工作流程

一个典型的 RAG 应用包含两个主要阶段：**数据索引阶段（离线）** 和 **检索生成阶段（在线）**。

### 1. 数据索引阶段 (Indexing)

这个阶段是预处理阶段，目的是将您的私有文档转换成一个可供快速检索的“知识库”（通常是向量数据库）。

![RAG Indexing Pipeline](https://.../rag-indexing.png)  <!-- 以后可以替换为真实的图表链接 -->

它包含以下步骤：

-   **a. 加载 (Load)**：
    从数据源（如 PDF, HTML, Notion, ...）加载文档。
-   **b. 分割 (Split)**：
    将长文档分割成小的、语义独立的文本块 (Chunks)。这一步至关重要，因为：
    -   太大的块会包含太多噪声，影响检索精度。
    -   太小的块则可能丢失关键的上下文信息。
    选择合适的块大小 (Chunk Size) 和重叠 (Overlap) 是 RAG 优化的关键点之一。
-   **c. 嵌入 (Embed)**：
    使用**嵌入模型 (Embedding Model)** 将每个文本块转换为一个高维的数字向量。这个向量可以被认为是文本块在语义空间中的“坐标”。语义上相似的文本块，其向量在空间中的距离也更近。
-   **d. 存储 (Store)**：
    将所有文本块及其对应的嵌入向量存入一个**向量数据库 (Vector Database)** 中。向量数据库专门为高效的向量相似性搜索而设计，可以快速找到与给定查询向量最相似的向量。

### 2. 检索生成阶段 (Retrieval & Generation)

这个阶段是用户与系统实时交互的阶段，当用户提出问题时触发。

![RAG Retrieval Pipeline](https://.../rag-retrieval.png) <!-- 以后可以替换为真实的图表链接 -->

它包含以下步骤：

-   **a. 用户提问 (User Query)**：
    用户输入一个问题（例如：“公司最新的报销政策是什么？”）。
-   **b. 查询嵌入 (Embed Query)**：
    使用与索引阶段**相同的嵌入模型**，将用户的问题也转换为一个查询向量。
-   **c. 向量检索 (Retrieve)**：
    在向量数据库中，使用这个查询向量进行相似性搜索，找出与问题向量最“接近”的 N 个文本块向量，并将它们对应的原始文本块作为“上下文 (Context)”返回。
-   **d. 增强提示词 (Augment Prompt)**：
    将用户原始的问题和上一步检索到的上下文，一起组合成一个新的、信息更丰富的提示词。这个提示词模板通常看起来像这样：
    ```
    请根据以下提供的上下文来回答用户的问题。

    上下文: 
    [这里是检索到的 N 个文本块...]

    问题: 
    [用户的原始问题]
    ```
-   **e. 生成答案 (Generate)**：
    最后，将这个增强后的提示词发送给大语言模型 (LLM)。LLM 会基于其强大的理解和生成能力，结合提供的上下文，生成一个精准、忠于事实的答案。

## 总结

RAG 通过一个“检索-阅读-回答”的模式，巧妙地将 LLM 强大的语言能力与外部知识库的准确性结合起来，有效地扩展了 LLM 的能力边界。理解其“索引”和“检索生成”两大阶段的工作原理，是构建和优化高级 AI 应用的基石。

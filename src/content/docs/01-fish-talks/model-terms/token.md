---
title: 'token'
description: '介绍 token 的定义、长度计算与常见用法'
---

# 为什么要了解 token

- 关系到费用、速度与是否超出上下文窗口。
- 影响提示词设计与拆分策略，决定能否稳定跑长任务。

# token 是什么

token 可以理解为模型处理文本时使用的最小单位，类似"字节与字符"的关系。不同模型使用不同的分词器（tokenizer），会把文本切成一个个 token 再进行计算与推理。

## 如何计算 token

- **英文**：大致 _4 个字符 ≈ 1 token_。例如 `hello` 可能被分为 `hel` + `lo` 两个 token。
- **数字/符号**：通常按块切分，如 `2025`、`http`、`://` 可能是不同 token。
- **模型差异**：不同模型/版本的分词器不同，同一段文本的 token 数可能略有差异。

精确计算需使用官方或社区分词器（如 OpenAI tiktoken、Anthropic tokenizer、SentencePiece 等）。

## 中文与 token 的差异

- 中文多为按字或常见词组切分，**1 个汉字通常≈1 个 token**（但并非绝对）。
- 中英文夹杂时，标点与空格也会占 token；**Markdown/JSON 等结构字符**同样计入。

## 上下文长度与费用

- **上下文长度（context window）**：模型一次可处理的输入+历史+输出 token 总上限。例如 128k/200k 等。
- **费用**：大多数供应商按输入 token 与输出 token 分别计费；部分模型还区分"推理/思考"token。
- **速度**：token 多意味着推理时间更长、流式返回更慢。

## 快速估算与示例

经验估算：

- 英文：_字符数 ÷ 4 ≈ token_；中文：_汉字数 ≈ token_。
- 段落中若包含大量格式字符（换行、JSON、表格），可再加 10%~30% 余量。

示例：约 1,000 字的中文文章，token ≈ 1,000～1,300；若加上系统提示与历史对话，很容易超过 2k。

## 使用建议

- **控制提示词体积**：把稳定不变的说明做成短标签或引用，而非每次粘贴长段文本。
- **结构化输入**：JSON/YAML 会增加标点 token，但能减少歧义，整体效果通常更好。
- **分批/检索**：长文处理考虑分片与检索增强（RAG），避免一次性把全文塞进上下文。
- **估算再调用**：关键任务前先本地估一遍 token，避免超限或费用突增。

---
title: 原理概述
description: 融合 RAG 与向量存储的核心原理、关键组件与设计取舍
---

## RAG 工作流程回顾

**RAG (检索增强生成)** 是一种让 AI 模型"带着知识说话"的技术范式。它通过在生成回答之前先从外部知识库检索相关信息,从而克服模型知识滞后和幻觉问题。

完整的 RAG 工作流程包括：

1. **索引阶段**：将文档分块、向量化并存储到向量数据库
2. **检索阶段**：将用户查询向量化，在知识库中检索相似内容
3. **重排序阶段**（可选）：使用 Cross-Encoder 等模型对检索结果进行精排
4. **上下文注入**：将检索到的相关内容与用户问题一起注入到 LLM
5. **生成阶段**：LLM 基于检索到的上下文生成带来源引用的回答

## 向量与嵌入技术

### 什么是向量嵌入？

在传统数据库中，我们通过精确匹配关键词来查询。但 AI 需要理解**语义相似性**——即使用不同的词语，也能找到意思相近的内容。

**向量嵌入 (Vector Embeddings)** 是将文本、图片等非结构化数据转换为高维数值向量的技术。语义相近的内容，其向量在空间中的距离也更近。

### 嵌入模型选择要点

- **多语言支持**：中文场景需选择对中文优化的模型（如 BGE、M3E 系列）
- **领域适配**：通用模型 vs 垂直领域微调模型（如法律、医疗专用）
- **向量维度**：高维度（1536）更精确但占用空间大，低维度（384）更快但精度略降
- **性能权衡**：Recall（召回率）vs Latency（延迟）vs 存储成本

常见嵌入模型：

- **OpenAI text-embedding-3-small/large**：商业API，质量高但有成本
- **BGE (BAAI General Embedding)**：中英文效果优秀的开源模型
- **M3E**：专为中文优化的开源嵌入模型
- **Sentence Transformers**：多种预训练模型可选

## 向量索引与近似检索

### 为什么需要近似检索？

在海量数据中进行精确的向量相似度计算成本极高。**近似最近邻 (ANN, Approximate Nearest Neighbor)** 算法通过牺牲一点点精确性，换取检索速度的数千倍提升。

### 主流 ANN 算法

- **HNSW (Hierarchical Navigable Small World)**
  - 基于图的算法，性能优异
  - 是目前许多主流向量数据库的首选（如 Qdrant、Weaviate）
  - 优点：召回率高、查询速度快
  - 缺点：构建索引较慢、内存占用大

- **IVF (Inverted File)**
  - 基于聚类的算法，将向量分组
  - 搜索时只在相关的组内进行，大幅减少计算量
  - 适合超大规模数据集

- **PQ (Product Quantization)**
  - 向量压缩技术，可与 IVF 结合使用
  - 显著降低存储和内存占用
  - 适合资源受限场景

### Recall-Latency 权衡

在实际应用中需要平衡：

- **高召回率**：找到更多相关结果，但查询更慢
- **低延迟**：响应更快，但可能遗漏部分相关内容
- 通过调整索引参数（如 HNSW 的 ef_construction、M 参数）来优化

## 检索策略

### 基础检索方法

- **kNN (k-Nearest Neighbors)**：返回最相似的 k 个向量
- **相似度阈值过滤**：只返回相似度超过阈值的结果
- **距离度量**：余弦相似度、欧式距离、点积等

### Hybrid 混合检索

单纯的向量检索可能遗漏精确匹配的关键词。**Hybrid Search** 结合了：

- **Dense Retrieval（密集检索）**：基于向量语义相似度
- **Sparse Retrieval（稀疏检索）**：基于关键词匹配（如 BM25）

通过加权融合两种结果，既能捕获语义，又能保证关键词命中。

### 重排序 (Reranking)

初步检索返回的结果可能排序不够精确。使用 **Cross-Encoder** 或专用 **Reranker 模型**：

- 对检索结果进行精细的相关性评分
- 显著提高最终呈现给 LLM 的上下文质量
- 常见工具：Cohere Rerank API、BGE Reranker 等

## 文档分块策略

### 为什么需要分块？

LLM 的上下文窗口有限，不能直接输入整篇文档。同时，向量检索在较小粒度上更精确。

### 分块方法

- **固定窗口分块**：按字符数或 Token 数切分（如每 512 tokens 一块）
- **重叠分块 (Chunk Overlap)**：相邻块之间有重叠部分，避免关键信息被截断
- **语义分块**：基于段落、句子边界进行智能切分
- **结构化分块**：针对代码、表格等结构化内容的特殊处理

### Chunk 大小与命中率的关系

- **Chunk 太小**：上下文不完整，信息碎片化
- **Chunk 太大**：不精确，检索到的内容中噪音多
- **经验值**：一般在 256-1024 tokens 之间，需根据实际数据调优

> 💡 **一句话建议**：切得太细或太粗都不好，根据文档类型和业务场景实测调优。

## 上下文注入模式

检索到相关内容后，如何将其注入到 LLM 提示词中？

### 常见模式

- **Stuff**：将所有检索到的内容直接拼接到提示词中
  - 简单直接，适合检索结果少的场景
- **Map-Reduce**：先对每个检索片段分别生成答案，再汇总
  - 适合处理大量文档，但会增加 API 调用次数

- **Refine**：逐个处理检索片段，不断精炼答案
  - 适合需要逐步推理的复杂问题

### 防止幻觉与事实一致性

- **显式来源引用**：要求 LLM 在回答中标注信息来源
- **事实校验**：对比生成内容与原文是否一致
- **置信度评分**：让模型评估自己的回答可信度

## 评估与监控

### 检索质量评估

- **Recall@K**：Top-K 结果中包含相关文档的比例
- **MRR (Mean Reciprocal Rank)**：第一个相关结果的排名倒数的平均值
- **NDCG (Normalized Discounted Cumulative Gain)**：考虑排序位置的综合指标

### 答案质量评估

- **答案忠实度 (Faithfulness)**：生成内容是否忠实于检索到的文档
- **答案相关性 (Relevancy)**：回答是否真正解决了用户问题
- **上下文精确度 (Context Precision)**：检索到的内容是否都是必要的

### 自动化评估框架

- **RAGAS**：专门用于 RAG 系统的评估工具集
- **LangChain Evaluators**：内置的多种评估器
- **自定义评估管道**：基于业务场景设计专属指标

---

> 💡 **核心要点**：RAG 和向量存储不是独立的技术，而是知识库系统的"一体两面"——RAG 是工作流程，向量数据库是技术基座。理解这些原理，是构建可靠 AI 应用的关键第一步。

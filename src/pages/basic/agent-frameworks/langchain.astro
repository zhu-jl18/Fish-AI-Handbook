---
import ContentLayout from '../../../layouts/ContentPageLayout.astro';

const section = 'basic';
const currentPage = 'agent-frameworks/langchain';
const title = 'LangChainæ¡†æ¶ - æ„å»ºè¯­è¨€æ¨¡å‹åº”ç”¨çš„æ ‡å‡†å¹³å°';
const sidebarContent = `
  <a href="/basic/agent-frameworks">Agentæ¡†æ¶æ¦‚è¿°</a>
  <a href="/basic/agent-frameworks/langchain" class="active">LangChainæ¡†æ¶</a>
  <a href="/basic/agent-frameworks/crewai">CrewAIå¤šæ™ºèƒ½ä½“</a>
`;
const headings = [
  { id: 'introduction', text: 'ä»‹ç»', depth: 1 },
  { id: 'core-architecture', text: 'æ ¸å¿ƒæ¶æ„', depth: 1 },
  { id: 'installation-setup', text: 'å®‰è£…ä¸é…ç½®', depth: 1 },
  { id: 'basic-usage', text: 'åŸºç¡€ä½¿ç”¨', depth: 1 },
  { id: 'advanced-features', text: 'é«˜çº§åŠŸèƒ½', depth: 1 },
  { id: 'real-world-examples', text: 'å®æˆ˜æ¡ˆä¾‹', depth: 1 },
  { id: 'best-practices', text: 'æœ€ä½³å®è·µ', depth: 1 },
  { id: 'ecosystem', text: 'ç”Ÿæ€ç³»ç»Ÿ', depth: 1 }
];
---

<ContentLayout 
  {title}
  {section}
  {currentPage}
  {sidebarContent}
  {headings}
>

## ä»‹ç»

LangChainæ˜¯ç›®å‰æœ€æµè¡Œå’Œæˆç†Ÿçš„å¤§è¯­è¨€æ¨¡å‹åº”ç”¨å¼€å‘æ¡†æ¶ï¼Œå®ƒæä¾›äº†å®Œæ•´çš„å·¥å…·é“¾å’ŒæŠ½è±¡å±‚ï¼Œè®©å¼€å‘è€…èƒ½å¤Ÿå¿«é€Ÿæ„å»ºå¤æ‚çš„AIåº”ç”¨ã€‚è‡ª2022å¹´å‘å¸ƒä»¥æ¥ï¼ŒLangChainå·²ç»æˆä¸ºAgentå¼€å‘é¢†åŸŸçš„äº‹å®æ ‡å‡†ã€‚

**æ ¸å¿ƒç‰¹ç‚¹**:
- ğŸ”— **é“¾å¼ç»„åˆ**: é€šè¿‡é“¾å¼è°ƒç”¨ç»„åˆå¤æ‚åŠŸèƒ½
- ğŸ§  **æ¨¡å‹æ— å…³**: æ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹æä¾›å•†
- ğŸ› ï¸ **ä¸°å¯Œå·¥å…·**: å†…ç½®å¤§é‡é¢„æ„å»ºç»„ä»¶
- ğŸ“š **æ–‡æ¡£å®Œå–„**: è¯¦å°½çš„æ–‡æ¡£å’Œæ•™ç¨‹
- ğŸŒ **ç¤¾åŒºæ´»è·ƒ**: åºå¤§çš„å¼€å‘è€…ç¤¾åŒº

**å‘å±•å†ç¨‹**:
- **2022å¹´10æœˆ**: Harrison Chaseåˆ›å»ºé¡¹ç›®
- **2023å¹´Q1**: å¿«é€Ÿå¢é•¿ï¼Œè·å¾—å¹¿æ³›å…³æ³¨
- **2023å¹´Q2**: å‘å¸ƒLangSmithä¼ä¸šçº§å·¥å…·
- **2023å¹´Q3**: æ¨å‡ºLangServeéƒ¨ç½²å¹³å°
- **2024å¹´**: ç”Ÿæ€ç³»ç»Ÿæˆç†Ÿï¼Œä¼ä¸šçº§åº”ç”¨å¹¿æ³›

## æ ¸å¿ƒæ¶æ„

### åŸºç¡€ç»„ä»¶å±‚æ¬¡
```mermaid
graph TB
    A[åº”ç”¨å±‚] --> B[é“¾å’Œä»£ç†å±‚]
    B --> C[æ ¸å¿ƒç»„ä»¶å±‚]
    C --> D[åŸºç¡€è®¾æ–½å±‚]
    
    B --> E[Chains]
    B --> F[Agents]
    B --> G[Memory]
    
    C --> H[LLMs]
    C --> I[Prompts]
    C --> J[Output Parsers]
    C --> K[Document Loaders]
    
    D --> L[Vector Stores]
    D --> M[Retrievers] 
    D --> N[Tools]
    D --> O[Utilities]
```

### æ ¸å¿ƒæŠ½è±¡æ¦‚å¿µ

#### 1. LLMæŠ½è±¡
```python
# LangChainçš„LLMæŠ½è±¡
from langchain.llms import OpenAI, Anthropic, HuggingFacePipeline

# ç»Ÿä¸€æ¥å£ï¼Œå¯ä»¥è½»æ¾åˆ‡æ¢æ¨¡å‹
llm = OpenAI(temperature=0.7)
# llm = Anthropic(model="claude-2")
# llm = HuggingFacePipeline(model_id="microsoft/DialoGPT-medium")

response = llm("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
```

#### 2. æç¤ºæ¨¡æ¿ (PromptTemplate)
```python
from langchain import PromptTemplate

# åˆ›å»ºå¯é‡ç”¨çš„æç¤ºæ¨¡æ¿
template = """
ä½ æ˜¯ä¸€ä¸ª{role}ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

èƒŒæ™¯ä¿¡æ¯ï¼š{context}
é—®é¢˜ï¼š{question}

å›ç­”ï¼š
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["role", "context", "question"]
)

# ä½¿ç”¨æ¨¡æ¿
formatted_prompt = prompt.format(
    role="è½¯ä»¶å·¥ç¨‹å¸ˆ",
    context="æ­£åœ¨å¼€å‘ä¸€ä¸ªç”µå•†ç½‘ç«™",
    question="å¦‚ä½•è®¾è®¡ç”¨æˆ·è®¤è¯ç³»ç»Ÿï¼Ÿ"
)
```

#### 3. é“¾ (Chains)
```python
from langchain.chains import LLMChain, SimpleSequentialChain

# å•ä¸ªé“¾
llm_chain = LLMChain(
    llm=llm,
    prompt=prompt,
    verbose=True
)

# é¡ºåºé“¾ - å°†å¤šä¸ªé“¾è¿æ¥èµ·æ¥
chain1 = LLMChain(llm=llm, prompt=research_prompt)
chain2 = LLMChain(llm=llm, prompt=write_prompt)

overall_chain = SimpleSequentialChain(
    chains=[chain1, chain2],
    verbose=True
)

result = overall_chain.run("å†™ä¸€ç¯‡å…³äºæœºå™¨å­¦ä¹ çš„æ–‡ç« ")
```

#### 4. ä»£ç† (Agents)
```python
from langchain.agents import initialize_agent, AgentType
from langchain.tools import DuckDuckGoSearchRun, Calculator

# å®šä¹‰å·¥å…·
tools = [
    DuckDuckGoSearchRun(),
    Calculator()
]

# åˆ›å»ºä»£ç†
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# ä½¿ç”¨ä»£ç†
result = agent.run("æœç´¢ä»Šå¤©çš„å¤©æ°”ï¼Œç„¶åè®¡ç®—åæ°æ¸©åº¦è½¬æ‘„æ°æ¸©åº¦")
```

## å®‰è£…ä¸é…ç½®

### åŸºç¡€å®‰è£…
```bash
# å®‰è£…æ ¸å¿ƒåŒ…
pip install langchain

# å®‰è£…ç‰¹å®šé›†æˆ
pip install langchain-openai        # OpenAIé›†æˆ
pip install langchain-anthropic     # Anthropicé›†æˆ
pip install langchain-community     # ç¤¾åŒºå·¥å…·
pip install langchain-experimental  # å®éªŒæ€§åŠŸèƒ½

# å®‰è£…é¢å¤–ä¾èµ–
pip install faiss-cpu              # å‘é‡æœç´¢
pip install chromadb               # å‘é‡æ•°æ®åº“
pip install beautifulsoup4         # ç½‘é¡µè§£æ
pip install youtube-transcript-api # YouTubeè½¬å½•
```

### ç¯å¢ƒé…ç½®
```python
import os
from langchain.llms import OpenAI

# è®¾ç½®APIå¯†é’¥
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["ANTHROPIC_API_KEY"] = "your-anthropic-api-key"
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "your-hf-token"

# åˆ›å»ºé…ç½®æ–‡ä»¶
# ~/.langchain/config.yaml
config = {
    'default_llm': 'openai',
    'temperature': 0.7,
    'max_tokens': 1000,
    'verbose': True
}
```

### Dockeréƒ¨ç½²
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["python", "app.py"]
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  langchain-app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./data:/app/data
```

## åŸºç¡€ä½¿ç”¨

### ç®€å•é—®ç­”ç³»ç»Ÿ
```python
from langchain.llms import OpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# åˆ›å»ºé—®ç­”é“¾
template = """
é—®é¢˜ï¼š{question}
ç­”æ¡ˆï¼šè®©æˆ‘æ¥å¸®ä½ è§£ç­”è¿™ä¸ªé—®é¢˜ã€‚
"""

prompt = PromptTemplate(
    template=template,
    input_variables=["question"]
)

llm = OpenAI(temperature=0.7)
chain = LLMChain(llm=llm, prompt=prompt)

# æé—®
response = chain.run("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
print(response)
```

### æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain

# åŠ è½½æ–‡æ¡£
loader = TextLoader("document.txt")
documents = loader.load()

# æ–‡æœ¬åˆ†å‰²
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# åˆ›å»ºå‘é‡å­˜å‚¨
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(texts, embeddings)

# åˆ›å»ºé—®ç­”é“¾
qa_chain = load_qa_chain(llm, chain_type="stuff")

# æŸ¥è¯¢
query = "æ–‡æ¡£ä¸­æåˆ°äº†ä»€ä¹ˆå…³é”®æ¦‚å¿µï¼Ÿ"
docs = vectorstore.similarity_search(query)
response = qa_chain.run(input_documents=docs, question=query)
```

### èŠå¤©æœºå™¨äºº
```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# å¤šè½®å¯¹è¯
response1 = conversation.predict(input="ä½ å¥½ï¼Œæˆ‘æ˜¯å¼ ä¸‰")
print(f"AI: {response1}")

response2 = conversation.predict(input="æˆ‘åˆšæ‰è¯´æˆ‘å«ä»€ä¹ˆï¼Ÿ")
print(f"AI: {response2}")
```

## é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰å·¥å…·åˆ›å»º
```python
from langchain.tools import BaseTool
from typing import Optional, Type
from pydantic import BaseModel, Field

class WeatherInput(BaseModel):
    """å¤©æ°”æŸ¥è¯¢çš„è¾“å…¥æ¨¡å¼"""
    city: str = Field(description="è¦æŸ¥è¯¢å¤©æ°”çš„åŸå¸‚åç§°")

class WeatherTool(BaseTool):
    name = "weather_search"
    description = "æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"
    args_schema: Type[BaseModel] = WeatherInput
    
    def _run(self, city: str) -> str:
        # å®é™…çš„å¤©æ°”APIè°ƒç”¨é€»è¾‘
        return f"{city}çš„å¤©æ°”ï¼šæ™´å¤©ï¼Œ25Â°C"
    
    def _arun(self, city: str) -> str:
        raise NotImplementedError("WeatherToolä¸æ”¯æŒå¼‚æ­¥è°ƒç”¨")

# ä½¿ç”¨è‡ªå®šä¹‰å·¥å…·
weather_tool = WeatherTool()
tools = [weather_tool]

agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)
```

### å¤æ‚çš„RAGç³»ç»Ÿ
```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.chains import RetrievalQA

# æ··åˆæ£€ç´¢å™¨
bm25_retriever = BM25Retriever.from_documents(documents)
faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, faiss_retriever],
    weights=[0.5, 0.5]
)

# é«˜çº§RAGé“¾
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="map_reduce",
    retriever=ensemble_retriever,
    return_source_documents=True
)

# æŸ¥è¯¢
result = qa_chain({"query": "å¤æ‚çš„æŠ€æœ¯é—®é¢˜"})
print(f"ç­”æ¡ˆ: {result['result']}")
print(f"æ¥æº: {result['source_documents']}")
```

### å¤šæ™ºèƒ½ä½“åä½œ
```python
from langchain.agents import AgentExecutor
from langchain.agents.conversational.base import ConversationalAgent

# åˆ›å»ºä¸“ä¸šåŒ–ä»£ç†
research_agent = ConversationalAgent.from_llm_and_tools(
    llm=llm,
    tools=research_tools,
    system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶å‘˜"
)

writing_agent = ConversationalAgent.from_llm_and_tools(
    llm=llm, 
    tools=writing_tools,
    system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†™ä½œåŠ©æ‰‹"
)

# åä½œå·¥ä½œæµ
class CollaborativeWorkflow:
    def __init__(self, research_agent, writing_agent):
        self.research_agent = research_agent
        self.writing_agent = writing_agent
    
    def execute_project(self, topic):
        # ç ”ç©¶é˜¶æ®µ
        research_result = self.research_agent.run(
            f"ç ”ç©¶å…³äº{topic}çš„è¯¦ç»†ä¿¡æ¯"
        )
        
        # å†™ä½œé˜¶æ®µ
        article = self.writing_agent.run(
            f"åŸºäºè¿™äº›ç ”ç©¶ä¿¡æ¯å†™ä¸€ç¯‡æ–‡ç« ï¼š{research_result}"
        )
        
        return article

workflow = CollaborativeWorkflow(research_agent, writing_agent)
result = workflow.execute_project("äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•")
```

## å®æˆ˜æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ
```python
class IntelligentCustomerService:
    def __init__(self):
        self.llm = OpenAI(temperature=0.3)
        self.memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=2000
        )
        
        # çŸ¥è¯†åº“
        self.knowledge_base = self._setup_knowledge_base()
        
        # æ„å›¾è¯†åˆ«é“¾
        self.intent_chain = self._create_intent_chain()
        
        # å“åº”ç”Ÿæˆé“¾
        self.response_chain = self._create_response_chain()
    
    def _setup_knowledge_base(self):
        # åŠ è½½FAQæ–‡æ¡£
        loader = CSVLoader("faq.csv")
        documents = loader.load()
        
        # å‘é‡åŒ–å­˜å‚¨
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(documents, embeddings)
        
        return vectorstore.as_retriever()
    
    def handle_customer_query(self, query):
        # æ„å›¾è¯†åˆ«
        intent = self.intent_chain.run(query=query)
        
        # æ£€ç´¢ç›¸å…³ä¿¡æ¯
        relevant_docs = self.knowledge_base.get_relevant_documents(query)
        
        # ç”Ÿæˆå›å¤
        response = self.response_chain.run(
            query=query,
            intent=intent,
            context=relevant_docs,
            chat_history=self.memory.chat_memory
        )
        
        # æ›´æ–°è®°å¿†
        self.memory.save_context(
            {"input": query},
            {"output": response}
        )
        
        return response

# ä½¿ç”¨å®¢æœç³»ç»Ÿ
customer_service = IntelligentCustomerService()
response = customer_service.handle_customer_query("æˆ‘çš„è®¢å•ä»€ä¹ˆæ—¶å€™å‘è´§ï¼Ÿ")
```

### æ¡ˆä¾‹2ï¼šä»£ç å®¡æŸ¥åŠ©æ‰‹
```python
class CodeReviewAssistant:
    def __init__(self):
        self.llm = OpenAI(temperature=0.2)
        
        # ä»£ç åˆ†æå·¥å…·
        self.tools = [
            CodeAnalysisTool(),
            SecurityScanTool(), 
            PerformanceTool(),
            StyleCheckTool()
        ]
        
        self.agent = initialize_agent(
            self.tools,
            self.llm,
            agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True
        )
    
    def review_code(self, code_content, language="python"):
        review_prompt = f"""
        è¯·å¯¹ä»¥ä¸‹{language}ä»£ç è¿›è¡Œå…¨é¢å®¡æŸ¥ï¼š
        
        {code_content}
        
        è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
        1. ä»£ç è´¨é‡å’Œå¯è¯»æ€§
        2. å®‰å…¨æ€§é—®é¢˜
        3. æ€§èƒ½ä¼˜åŒ–å»ºè®®
        4. æœ€ä½³å®è·µéµå¾ªæƒ…å†µ
        """
        
        review_result = self.agent.run(review_prompt)
        return review_result

# ä½¿ç”¨ä»£ç å®¡æŸ¥åŠ©æ‰‹
code_reviewer = CodeReviewAssistant()
code = """
def calculate_total(items):
    total = 0
    for item in items:
        total += item.price * item.quantity
    return total
"""

review = code_reviewer.review_code(code)
print(review)
```

### æ¡ˆä¾‹3ï¼šå†…å®¹è¥é”€è‡ªåŠ¨åŒ–
```python
class ContentMarketingPipeline:
    def __init__(self):
        self.llm = OpenAI(temperature=0.8)
        
        # ç ”ç©¶é˜¶æ®µ
        self.research_chain = self._create_research_chain()
        
        # å†…å®¹åˆ›ä½œé˜¶æ®µ
        self.content_chain = self._create_content_chain()
        
        # SEOä¼˜åŒ–é˜¶æ®µ
        self.seo_chain = self._create_seo_chain()
        
        # ç¤¾äº¤åª’ä½“é€‚é…
        self.social_chain = self._create_social_chain()
    
    def create_marketing_content(self, topic, target_audience):
        # 1. ç ”ç©¶é˜¶æ®µ
        research_data = self.research_chain.run(
            topic=topic,
            audience=target_audience
        )
        
        # 2. å†…å®¹åˆ›ä½œ
        blog_post = self.content_chain.run(
            topic=topic,
            research_data=research_data,
            audience=target_audience
        )
        
        # 3. SEOä¼˜åŒ–
        optimized_content = self.seo_chain.run(
            content=blog_post,
            topic=topic
        )
        
        # 4. ç¤¾äº¤åª’ä½“é€‚é…
        social_posts = self.social_chain.run(
            content=optimized_content,
            platforms=["twitter", "linkedin", "facebook"]
        )
        
        return {
            "blog_post": optimized_content,
            "social_media": social_posts,
            "research_summary": research_data
        }

# ä½¿ç”¨è¥é”€æµæ°´çº¿
marketing_pipeline = ContentMarketingPipeline()
content_package = marketing_pipeline.create_marketing_content(
    topic="AIåœ¨åŒ»ç–—å¥åº·é¢†åŸŸçš„åº”ç”¨",
    target_audience="åŒ»ç–—ä¸“ä¸šäººå£«"
)
```

## æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–
```python
# ä½¿ç”¨ç¼“å­˜å‡å°‘APIè°ƒç”¨
from langchain.cache import InMemoryCache
import langchain
langchain.llm_cache = InMemoryCache()

# å¼‚æ­¥å¤„ç†æé«˜å¹¶å‘æ€§
from langchain.callbacks.manager import AsyncCallbackManagerForLLMRun
import asyncio

async def batch_process_queries(queries):
    tasks = [llm.agenerate([query]) for query in queries]
    results = await asyncio.gather(*tasks)
    return results

# ä½¿ç”¨æµå¼è¾“å‡ºæ”¹å–„ç”¨æˆ·ä½“éªŒ
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = OpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
    temperature=0.7
)
```

### 2. é”™è¯¯å¤„ç†å’Œé‡è¯•
```python
from langchain.llms.base import LLM
from tenacity import retry, stop_after_attempt, wait_exponential

class RobustLLM(LLM):
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def _call(self, prompt: str, stop=None, run_manager=None):
        try:
            return self.llm._call(prompt, stop, run_manager)
        except Exception as e:
            print(f"APIè°ƒç”¨å¤±è´¥: {e}")
            raise

# ä½¿ç”¨å¥å£®çš„LLMåŒ…è£…å™¨
robust_llm = RobustLLM()
```

### 3. ç›‘æ§å’Œæ—¥å¿—
```python
from langchain.callbacks import StdOutCallbackHandler
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class CustomCallbackHandler(StdOutCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"LLMå¼€å§‹å¤„ç†: {len(prompts)}ä¸ªæç¤º")
        
    def on_llm_end(self, response, **kwargs):
        print(f"LLMå®Œæˆå¤„ç†: {response.generations}")
        
    def on_llm_error(self, error, **kwargs):
        print(f"LLMå¤„ç†é”™è¯¯: {error}")

# ä½¿ç”¨è‡ªå®šä¹‰å›è°ƒ
chain = LLMChain(
    llm=llm,
    prompt=prompt,
    callbacks=[CustomCallbackHandler()]
)
```

## ç”Ÿæ€ç³»ç»Ÿ

### LangSmith - ä¼ä¸šçº§å¼€å‘å¹³å°
```python
from langsmith import Client
from langchain.smith import RunEvalConfig, run_on_dataset

# LangSmithå®¢æˆ·ç«¯
client = Client()

# åˆ›å»ºæ•°æ®é›†
dataset = client.create_dataset("qa-evaluation")

# è¯„ä¼°é“¾çš„æ€§èƒ½
eval_config = RunEvalConfig(
    evaluators=["qa", "criteria"],
    custom_evaluators=[],
)

results = run_on_dataset(
    dataset_name="qa-evaluation",
    llm_or_chain_factory=lambda: qa_chain,
    evaluation=eval_config,
    verbose=True,
)
```

### LangServe - éƒ¨ç½²å¹³å°
```python
from langserve import add_routes
from fastapi import FastAPI

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="LangChainæœåŠ¡å™¨",
    version="1.0",
    description="åŸºäºLangChainçš„APIæœåŠ¡å™¨",
)

# æ·»åŠ é“¾è·¯ç”±
add_routes(
    app,
    qa_chain,
    path="/qa",
)

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="localhost", port=8000)
```

### ç¤¾åŒºè´¡çŒ®å’Œæ‰©å±•
```python
# è´¡çŒ®è‡ªå®šä¹‰ç»„ä»¶åˆ°ç¤¾åŒº
from langchain.tools import BaseTool

class CustomTool(BaseTool):
    """è‡ªå®šä¹‰å·¥å…·ç¤ºä¾‹"""
    name = "custom_tool"
    description = "è¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰å·¥å…·çš„ç¤ºä¾‹"
    
    def _run(self, query: str) -> str:
        # å·¥å…·çš„æ ¸å¿ƒé€»è¾‘
        return f"å¤„ç†ç»“æœ: {query}"
    
    def _arun(self, query: str) -> str:
        raise NotImplementedError()

# å‘å¸ƒåˆ°ç¤¾åŒº
# 1. åˆ›å»ºåŒ…ç»“æ„
# 2. ç¼–å†™æ–‡æ¡£
# 3. æäº¤PRåˆ°langchain-community
# 4. ç¤¾åŒºå®¡æŸ¥å’Œåˆå¹¶
```

LangChainä½œä¸ºAgentå¼€å‘çš„æ ‡æ†æ¡†æ¶ï¼Œä¸ä»…æä¾›äº†å¼ºå¤§çš„åŠŸèƒ½ï¼Œæ›´å»ºç«‹äº†å®Œæ•´çš„å¼€å‘ç”Ÿæ€ç³»ç»Ÿã€‚æŒæ¡LangChainï¼Œå°±æ˜¯æŒæ¡äº†ç°ä»£AIåº”ç”¨å¼€å‘çš„æ ¸å¿ƒæŠ€èƒ½ã€‚æ— è®ºæ˜¯ç®€å•çš„åŸå‹å¼€å‘è¿˜æ˜¯å¤æ‚çš„ä¼ä¸šçº§åº”ç”¨ï¼ŒLangChainéƒ½æä¾›äº†ç›¸åº”çš„è§£å†³æ–¹æ¡ˆå’Œæœ€ä½³å®è·µã€‚

</ContentLayout>

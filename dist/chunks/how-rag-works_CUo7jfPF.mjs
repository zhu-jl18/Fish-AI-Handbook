import { c as createComponent, m as maybeRenderHead, u as unescapeHTML, a as renderTemplate } from './astro/server_DMTDEdEd.mjs';
import 'kleur/colors';
import 'clsx';

const html = "<h2 id=\"rag-的诞生为何需要它\">RAG 的诞生：为何需要它？</h2>\n<p>大语言模型（LLM）虽然知识渊博，但面临两大核心挑战：</p>\n<ol>\n<li><strong>知识截止 (Knowledge Cutoff)</strong>：模型的知识停留在训练数据的某个时间点，无法获取最新信息。</li>\n<li><strong>幻觉 (Hallucination)</strong>：模型有时会“一本正经地胡说八道”，编造一些看似合理但实际错误的信息，尤其是在处理其知识范围外或需要高事实性的问题时。</li>\n</ol>\n<p><strong>RAG (检索增强生成)</strong> 正是为了解决这些问题而设计的框架。它的核心思想非常直观：<strong>不让模型仅凭记忆回答，而是给它一本书（外部知识库）让它边查边答。</strong></p>\n<p>通过在生成答案之前，先从一个最新的、可信的知识库中检索相关信息，RAG 能够显著提升答案的准确性、时效性，并有效抑制幻觉。</p>\n<h2 id=\"rag-的核心工作流程\">RAG 的核心工作流程</h2>\n<p>一个典型的 RAG 应用包含两个主要阶段：<strong>数据索引阶段（离线）</strong> 和 <strong>检索生成阶段（在线）</strong>。</p>\n<h3 id=\"1-数据索引阶段-indexing\">1. 数据索引阶段 (Indexing)</h3>\n<p>这个阶段是预处理阶段，目的是将您的私有文档转换成一个可供快速检索的“知识库”（通常是向量数据库）。</p>\n<p><img src=\"https://.../rag-indexing.png\" alt=\"RAG Indexing Pipeline\">  <!-- 以后可以替换为真实的图表链接 --></p>\n<p>它包含以下步骤：</p>\n<ul>\n<li><strong>a. 加载 (Load)</strong>：\r\n从数据源（如 PDF, HTML, Notion, …）加载文档。</li>\n<li><strong>b. 分割 (Split)</strong>：\r\n将长文档分割成小的、语义独立的文本块 (Chunks)。这一步至关重要，因为：\n<ul>\n<li>太大的块会包含太多噪声，影响检索精度。</li>\n<li>太小的块则可能丢失关键的上下文信息。\r\n选择合适的块大小 (Chunk Size) 和重叠 (Overlap) 是 RAG 优化的关键点之一。</li>\n</ul>\n</li>\n<li><strong>c. 嵌入 (Embed)</strong>：\r\n使用<strong>嵌入模型 (Embedding Model)</strong> 将每个文本块转换为一个高维的数字向量。这个向量可以被认为是文本块在语义空间中的“坐标”。语义上相似的文本块，其向量在空间中的距离也更近。</li>\n<li><strong>d. 存储 (Store)</strong>：\r\n将所有文本块及其对应的嵌入向量存入一个<strong>向量数据库 (Vector Database)</strong> 中。向量数据库专门为高效的向量相似性搜索而设计，可以快速找到与给定查询向量最相似的向量。</li>\n</ul>\n<h3 id=\"2-检索生成阶段-retrieval--generation\">2. 检索生成阶段 (Retrieval &#x26; Generation)</h3>\n<p>这个阶段是用户与系统实时交互的阶段，当用户提出问题时触发。</p>\n<p><img src=\"https://.../rag-retrieval.png\" alt=\"RAG Retrieval Pipeline\"> <!-- 以后可以替换为真实的图表链接 --></p>\n<p>它包含以下步骤：</p>\n<ul>\n<li><strong>a. 用户提问 (User Query)</strong>：\r\n用户输入一个问题（例如：“公司最新的报销政策是什么？”）。</li>\n<li><strong>b. 查询嵌入 (Embed Query)</strong>：\r\n使用与索引阶段<strong>相同的嵌入模型</strong>，将用户的问题也转换为一个查询向量。</li>\n<li><strong>c. 向量检索 (Retrieve)</strong>：\r\n在向量数据库中，使用这个查询向量进行相似性搜索，找出与问题向量最“接近”的 N 个文本块向量，并将它们对应的原始文本块作为“上下文 (Context)”返回。</li>\n<li><strong>d. 增强提示词 (Augment Prompt)</strong>：\r\n将用户原始的问题和上一步检索到的上下文，一起组合成一个新的、信息更丰富的提示词。这个提示词模板通常看起来像这样：\n<pre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>请根据以下提供的上下文来回答用户的问题。</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>上下文: </span></span>\n<span class=\"line\"><span>[这里是检索到的 N 个文本块...]</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>问题: </span></span>\n<span class=\"line\"><span>[用户的原始问题]</span></span>\n<span class=\"line\"><span></span></span></code></pre>\n</li>\n<li><strong>e. 生成答案 (Generate)</strong>：\r\n最后，将这个增强后的提示词发送给大语言模型 (LLM)。LLM 会基于其强大的理解和生成能力，结合提供的上下文，生成一个精准、忠于事实的答案。</li>\n</ul>\n<h2 id=\"总结\">总结</h2>\n<p>RAG 通过一个“检索-阅读-回答”的模式，巧妙地将 LLM 强大的语言能力与外部知识库的准确性结合起来，有效地扩展了 LLM 的能力边界。理解其“索引”和“检索生成”两大阶段的工作原理，是构建和优化高级 AI 应用的基石。</p>";

				const frontmatter = {"title":"RAG 工作原理解析","description":"深入剖析检索增强生成（RAG）的工作流程、核心组件与关键挑战。"};
				const file = "X:/Projcet/AI BOOK/src/content/docs/06-technical-deep-dive/how-rag-works.md";
				const url = undefined;
				function rawContent() {
					return "\r\n## RAG 的诞生：为何需要它？\r\n\r\n大语言模型（LLM）虽然知识渊博，但面临两大核心挑战：\r\n1.  **知识截止 (Knowledge Cutoff)**：模型的知识停留在训练数据的某个时间点，无法获取最新信息。\r\n2.  **幻觉 (Hallucination)**：模型有时会“一本正经地胡说八道”，编造一些看似合理但实际错误的信息，尤其是在处理其知识范围外或需要高事实性的问题时。\r\n\r\n**RAG (检索增强生成)** 正是为了解决这些问题而设计的框架。它的核心思想非常直观：**不让模型仅凭记忆回答，而是给它一本书（外部知识库）让它边查边答。**\r\n\r\n通过在生成答案之前，先从一个最新的、可信的知识库中检索相关信息，RAG 能够显著提升答案的准确性、时效性，并有效抑制幻觉。\r\n\r\n## RAG 的核心工作流程\r\n\r\n一个典型的 RAG 应用包含两个主要阶段：**数据索引阶段（离线）** 和 **检索生成阶段（在线）**。\r\n\r\n### 1. 数据索引阶段 (Indexing)\r\n\r\n这个阶段是预处理阶段，目的是将您的私有文档转换成一个可供快速检索的“知识库”（通常是向量数据库）。\r\n\r\n![RAG Indexing Pipeline](https://.../rag-indexing.png)  <!-- 以后可以替换为真实的图表链接 -->\r\n\r\n它包含以下步骤：\r\n\r\n-   **a. 加载 (Load)**：\r\n    从数据源（如 PDF, HTML, Notion, ...）加载文档。\r\n-   **b. 分割 (Split)**：\r\n    将长文档分割成小的、语义独立的文本块 (Chunks)。这一步至关重要，因为：\r\n    -   太大的块会包含太多噪声，影响检索精度。\r\n    -   太小的块则可能丢失关键的上下文信息。\r\n    选择合适的块大小 (Chunk Size) 和重叠 (Overlap) 是 RAG 优化的关键点之一。\r\n-   **c. 嵌入 (Embed)**：\r\n    使用**嵌入模型 (Embedding Model)** 将每个文本块转换为一个高维的数字向量。这个向量可以被认为是文本块在语义空间中的“坐标”。语义上相似的文本块，其向量在空间中的距离也更近。\r\n-   **d. 存储 (Store)**：\r\n    将所有文本块及其对应的嵌入向量存入一个**向量数据库 (Vector Database)** 中。向量数据库专门为高效的向量相似性搜索而设计，可以快速找到与给定查询向量最相似的向量。\r\n\r\n### 2. 检索生成阶段 (Retrieval & Generation)\r\n\r\n这个阶段是用户与系统实时交互的阶段，当用户提出问题时触发。\r\n\r\n![RAG Retrieval Pipeline](https://.../rag-retrieval.png) <!-- 以后可以替换为真实的图表链接 -->\r\n\r\n它包含以下步骤：\r\n\r\n-   **a. 用户提问 (User Query)**：\r\n    用户输入一个问题（例如：“公司最新的报销政策是什么？”）。\r\n-   **b. 查询嵌入 (Embed Query)**：\r\n    使用与索引阶段**相同的嵌入模型**，将用户的问题也转换为一个查询向量。\r\n-   **c. 向量检索 (Retrieve)**：\r\n    在向量数据库中，使用这个查询向量进行相似性搜索，找出与问题向量最“接近”的 N 个文本块向量，并将它们对应的原始文本块作为“上下文 (Context)”返回。\r\n-   **d. 增强提示词 (Augment Prompt)**：\r\n    将用户原始的问题和上一步检索到的上下文，一起组合成一个新的、信息更丰富的提示词。这个提示词模板通常看起来像这样：\r\n    ```\r\n    请根据以下提供的上下文来回答用户的问题。\r\n\r\n    上下文: \r\n    [这里是检索到的 N 个文本块...]\r\n\r\n    问题: \r\n    [用户的原始问题]\r\n    ```\r\n-   **e. 生成答案 (Generate)**：\r\n    最后，将这个增强后的提示词发送给大语言模型 (LLM)。LLM 会基于其强大的理解和生成能力，结合提供的上下文，生成一个精准、忠于事实的答案。\r\n\r\n## 总结\r\n\r\nRAG 通过一个“检索-阅读-回答”的模式，巧妙地将 LLM 强大的语言能力与外部知识库的准确性结合起来，有效地扩展了 LLM 的能力边界。理解其“索引”和“检索生成”两大阶段的工作原理，是构建和优化高级 AI 应用的基石。\r\n";
				}
				function compiledContent() {
					return html;
				}
				function getHeadings() {
					return [{"depth":2,"slug":"rag-的诞生为何需要它","text":"RAG 的诞生：为何需要它？"},{"depth":2,"slug":"rag-的核心工作流程","text":"RAG 的核心工作流程"},{"depth":3,"slug":"1-数据索引阶段-indexing","text":"1. 数据索引阶段 (Indexing)"},{"depth":3,"slug":"2-检索生成阶段-retrieval--generation","text":"2. 检索生成阶段 (Retrieval & Generation)"},{"depth":2,"slug":"总结","text":"总结"}];
				}

				const Content = createComponent((result, _props, slots) => {
					const { layout, ...content } = frontmatter;
					content.file = file;
					content.url = url;

					return renderTemplate`${maybeRenderHead()}${unescapeHTML(html)}`;
				});

export { Content, compiledContent, Content as default, file, frontmatter, getHeadings, rawContent, url };

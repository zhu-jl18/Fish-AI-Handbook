const id = "06-technical-deep-dive/how-rag-works.md";
						const collection = "docs";
						const slug = "06-technical-deep-dive/how-rag-works";
						const body = "\r\n## RAG 的诞生：为何需要它？\r\n\r\n大语言模型（LLM）虽然知识渊博，但面临两大核心挑战：\r\n1.  **知识截止 (Knowledge Cutoff)**：模型的知识停留在训练数据的某个时间点，无法获取最新信息。\r\n2.  **幻觉 (Hallucination)**：模型有时会“一本正经地胡说八道”，编造一些看似合理但实际错误的信息，尤其是在处理其知识范围外或需要高事实性的问题时。\r\n\r\n**RAG (检索增强生成)** 正是为了解决这些问题而设计的框架。它的核心思想非常直观：**不让模型仅凭记忆回答，而是给它一本书（外部知识库）让它边查边答。**\r\n\r\n通过在生成答案之前，先从一个最新的、可信的知识库中检索相关信息，RAG 能够显著提升答案的准确性、时效性，并有效抑制幻觉。\r\n\r\n## RAG 的核心工作流程\r\n\r\n一个典型的 RAG 应用包含两个主要阶段：**数据索引阶段（离线）** 和 **检索生成阶段（在线）**。\r\n\r\n### 1. 数据索引阶段 (Indexing)\r\n\r\n这个阶段是预处理阶段，目的是将您的私有文档转换成一个可供快速检索的“知识库”（通常是向量数据库）。\r\n\r\n![RAG Indexing Pipeline](https://.../rag-indexing.png)  <!-- 以后可以替换为真实的图表链接 -->\r\n\r\n它包含以下步骤：\r\n\r\n-   **a. 加载 (Load)**：\r\n    从数据源（如 PDF, HTML, Notion, ...）加载文档。\r\n-   **b. 分割 (Split)**：\r\n    将长文档分割成小的、语义独立的文本块 (Chunks)。这一步至关重要，因为：\r\n    -   太大的块会包含太多噪声，影响检索精度。\r\n    -   太小的块则可能丢失关键的上下文信息。\r\n    选择合适的块大小 (Chunk Size) 和重叠 (Overlap) 是 RAG 优化的关键点之一。\r\n-   **c. 嵌入 (Embed)**：\r\n    使用**嵌入模型 (Embedding Model)** 将每个文本块转换为一个高维的数字向量。这个向量可以被认为是文本块在语义空间中的“坐标”。语义上相似的文本块，其向量在空间中的距离也更近。\r\n-   **d. 存储 (Store)**：\r\n    将所有文本块及其对应的嵌入向量存入一个**向量数据库 (Vector Database)** 中。向量数据库专门为高效的向量相似性搜索而设计，可以快速找到与给定查询向量最相似的向量。\r\n\r\n### 2. 检索生成阶段 (Retrieval & Generation)\r\n\r\n这个阶段是用户与系统实时交互的阶段，当用户提出问题时触发。\r\n\r\n![RAG Retrieval Pipeline](https://.../rag-retrieval.png) <!-- 以后可以替换为真实的图表链接 -->\r\n\r\n它包含以下步骤：\r\n\r\n-   **a. 用户提问 (User Query)**：\r\n    用户输入一个问题（例如：“公司最新的报销政策是什么？”）。\r\n-   **b. 查询嵌入 (Embed Query)**：\r\n    使用与索引阶段**相同的嵌入模型**，将用户的问题也转换为一个查询向量。\r\n-   **c. 向量检索 (Retrieve)**：\r\n    在向量数据库中，使用这个查询向量进行相似性搜索，找出与问题向量最“接近”的 N 个文本块向量，并将它们对应的原始文本块作为“上下文 (Context)”返回。\r\n-   **d. 增强提示词 (Augment Prompt)**：\r\n    将用户原始的问题和上一步检索到的上下文，一起组合成一个新的、信息更丰富的提示词。这个提示词模板通常看起来像这样：\r\n    ```\r\n    请根据以下提供的上下文来回答用户的问题。\r\n\r\n    上下文: \r\n    [这里是检索到的 N 个文本块...]\r\n\r\n    问题: \r\n    [用户的原始问题]\r\n    ```\r\n-   **e. 生成答案 (Generate)**：\r\n    最后，将这个增强后的提示词发送给大语言模型 (LLM)。LLM 会基于其强大的理解和生成能力，结合提供的上下文，生成一个精准、忠于事实的答案。\r\n\r\n## 总结\r\n\r\nRAG 通过一个“检索-阅读-回答”的模式，巧妙地将 LLM 强大的语言能力与外部知识库的准确性结合起来，有效地扩展了 LLM 的能力边界。理解其“索引”和“检索生成”两大阶段的工作原理，是构建和优化高级 AI 应用的基石。\r\n";
						const data = {title:"RAG 工作原理解析",description:"深入剖析检索增强生成（RAG）的工作流程、核心组件与关键挑战。"};
						const _internal = {
							type: 'content',
							filePath: "X:/Projcet/AI BOOK/src/content/docs/06-technical-deep-dive/how-rag-works.md",
							rawData: undefined,
						};

export { _internal, body, collection, data, id, slug };
